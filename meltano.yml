default_environment: dev
project_id: 019b7916-fe65-7a10-9076-09291e966f99
include_paths:
- ./load/mongo-extract-jsondocs.yml
- ./load/mongo-extract-mapped.yml
environments:
- name: dev
# - name: staging
# - name: prod


plugins:
  loaders:
  - name: target-postgres
    variant: meltanolabs
    pip_url: meltanolabs-target-postgres
    config:
      database: carbon_lens_bi
      host: localhost
      port: 5432
      user: loader_admin
      ssl_mode: allow
      default_target_schema: rawcl
      # For INCREMENTAL replication with upsert:
      load_method: upsert             # Uses primary key to insert or update
      add_record_metadata: true       # Adds _sdc_extracted_at, _sdc_received_at
      activate_version: false

  - name: target-csv
    variant: meltanolabs
    pip_url: meltanolabs-target-csv
    config:
      output_path: ${MELTANO_PROJECT_ROOT}/output/csv
      file_naming_scheme: '{stream_name}-{timestamp}.csv'
      # overwrite_behavior: replace_file
      # add_record_metadata: true
      # Note: stream_maps already flatten, so flattening here is optional
      # flattening_enabled: false

  - name: target-duckdb
    variant: jwills
    pip_url: target-duckdb~=0.8
    config:
      path: ${MELTANO_PROJECT_ROOT}/output/duckdb/warehouse.duckdb
      default_target_schema: public
      add_metadata_columns: true
      # Note: stream_maps already flatten, no need for extra flattening
      data_flattening_max_level: 0
      batch_size_rows: 100000
      hard_delete: false


# ═══════════════════════════════════════════════════════════════════════════════
# NOTES
# ═══════════════════════════════════════════════════════════════════════════════
# 
# STRATEGY: envelope
#   - Wraps entire MongoDB document in a 'document' object field
#   - stream_maps extract individual fields using document.get('fieldName')
#   - Type casting: float(...) if ... else None for numeric fields
#   - document: __NULL__ removes the envelope from output
#   - __else__: __NULL__ removes any unmapped fields
#
# YAML ANCHORS:
#   - &anchor_name defines a reusable block
#   - <<: *anchor_name merges that block into current mapping
#   - ONLY ONE <<: merge allowed per mapping (YAML limitation)
#   - Additional fields after <<: extend/override anchor values
#
# COLLECTION NAMING:
#   - MongoDB: camelCase (e.g., stationaryCombustions)
#   - Stream name: carbonLens_stationarycombustions (lowercase)
#   - Output columns: snake_case (e.g., company_id, calculated_emission)
#
# EMPTY COLLECTIONS:
#   - Some collections may be empty (no schema deducible)
#   - stream_maps still work - they just won't produce output
#   - Model files define expected fields for future data
#
# USAGE:
#   meltano run tap-mongodb target-csv
#   meltano run tap-mongodb target-duckdb
#   meltano run tap-mongodb target-postgres
#
